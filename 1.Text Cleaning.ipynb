{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d3dc60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mkha7672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mkha7672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mkha7672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 0: Ensure necessary NLTK resources are downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('stopwords')  # For stopword removal\n",
    "nltk.download('wordnet')  # For lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3aa168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\mkha7672\\appdata\\local\\anaconda3\\lib\\site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ab73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\n",
    "    \"I loooove NLP!!! It's sooo coool. #nlp #fun @ai\",\n",
    "    \"Artificial Intelligence is the future of technology... $$$\",\n",
    "    \"Python is great for Data Science, and it's widely used in AI & ML.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb11c8",
   "metadata": {},
   "source": [
    "# Step 1: Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20865c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased documents:\n",
      " [\"i loooove nlp!!! it's sooo coool. #nlp #fun @ai\", 'artificial intelligence is the future of technology... $$$', \"python is great for data science, and it's widely used in ai & ml.\"]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Step 1: Convert to lowercase\n",
    "raw_docs = [doc.lower() for doc in raw_docs]\n",
    "print(\"Lowercased documents:\\n\", raw_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f74e18d",
   "metadata": {},
   "source": [
    "# Step 2: Tokenization (Word and Sentence Tokenization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad2dc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenized documents:\n",
      " [['i', 'loooove', 'nlp', '!', '!', '!', 'it', \"'s\", 'sooo', 'coool', '.', '#', 'nlp', '#', 'fun', '@', 'ai'], ['artificial', 'intelligence', 'is', 'the', 'future', 'of', 'technology', '...', '$', '$', '$'], ['python', 'is', 'great', 'for', 'data', 'science', ',', 'and', 'it', \"'s\", 'widely', 'used', 'in', 'ai', '&', 'ml', '.']]\n",
      "\n",
      "Sentence Tokenized documents:\n",
      " [['i loooove nlp!!!', \"it's sooo coool.\", '#nlp #fun @ai'], ['artificial intelligence is the future of technology... $$$'], [\"python is great for data science, and it's widely used in ai & ml.\"]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Word tokenization\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(\"Word Tokenized documents:\\n\", tokenized_docs)\n",
    "\n",
    "# Sentence tokenization\n",
    "sent_token = [sent_tokenize(doc) for doc in raw_docs]\n",
    "print(\"\\nSentence Tokenized documents:\\n\", sent_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe87590",
   "metadata": {},
   "source": [
    "# Step 3: Punctuation Removal\n",
    "Removing punctuation helps reduce noise in the text data.\n",
    "\n",
    "re.sub(), which is a function for substituting patterns in text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88de3245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents after punctuation removal:\n",
      " [['i', 'loooove', 'nlp', 'it', 's', 'sooo', 'coool', 'nlp', 'fun', 'ai'], ['artificial', 'intelligence', 'is', 'the', 'future', 'of', 'technology'], ['python', 'is', 'great', 'for', 'data', 'science', 'and', 'it', 's', 'widely', 'used', 'in', 'ai', 'ml']]\n"
     ]
    }
   ],
   "source": [
    "#using re to remove punctuation from text by finding and replacing any punctuation marks(like . , ? ! etc.) with an empty string.\n",
    "import re\n",
    "\n",
    "# Removing punctuation\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) #escapes any special characters\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token) # Replace punctuation with nothing\n",
    "        if new_token != u'':  # Only keep non-empty tokens after removing punctuation\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "\n",
    "print(\"\\nDocuments after punctuation removal:\\n\", tokenized_docs_no_punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12de22",
   "metadata": {},
   "source": [
    "# Step 4: Remove Stopwords\n",
    "We remove common words that don’t contribute much to the meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e6c1739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents after stopword removal:\n",
      " [['loooove', 'nlp', 'sooo', 'coool', 'nlp', 'fun', 'ai'], ['artificial', 'intelligence', 'future', 'technology'], ['python', 'great', 'data', 'science', 'widely', 'used', 'ai', 'ml']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = [word for word in doc if word not in stop_words]\n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(\"\\nDocuments after stopword removal:\\n\", tokenized_docs_no_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fd708",
   "metadata": {},
   "source": [
    "# Step 5: Lemmatization\n",
    "Lemmatization reduces words to their root form, ensuring they are valid dictionary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e95c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents after lemmatization:\n",
      " [['loooove', 'nlp', 'sooo', 'coool', 'nlp', 'fun', 'ai'], ['artificial', 'intelligence', 'future', 'technology'], ['python', 'great', 'data', 'science', 'widely', 'used', 'ai', 'ml']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization\n",
    "preprocessed_docs = []\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    final_doc = [wordnet_lemmatizer.lemmatize(word) for word in doc]\n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print(\"\\nDocuments after lemmatization:\\n\", preprocessed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b70e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents after stemming (Porter Stemmer):\n",
      " [['loooov', 'nlp', 'sooo', 'coool', 'nlp', 'fun', 'ai'], ['artifici', 'intellig', 'futur', 'technolog'], ['python', 'great', 'data', 'scienc', 'wide', 'use', 'ai', 'ml']]\n"
     ]
    }
   ],
   "source": [
    "#Using PorterStemmer for Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_docs = []\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    final_doc = [porter_stemmer.stem(word) for word in doc]\n",
    "    stemmed_docs.append(final_doc)\n",
    "\n",
    "print(\"\\nDocuments after stemming (Porter Stemmer):\\n\", stemmed_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93819822",
   "metadata": {},
   "source": [
    "Stemming is good when you need speed and don’t care too much about whether the root word is a valid word. It's often used in information retrieval and search engines.\n",
    "\n",
    "Lemmatization is preferred when you need accuracy and want to retain the correct word forms. It is better for tasks like machine translation or where the output needs to be human-readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998211fc",
   "metadata": {},
   "source": [
    "# Advanced Steps\n",
    "Spell Checking (Optional) Spell checking can correct misspelled words like \"loooove\" and \"sooo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed8da61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original documents:\n",
      "['loooove', 'nlp', 'sooo', 'coool', 'nlp', 'fun', 'ai']\n",
      "['artificial', 'intelligence', 'future', 'technology']\n",
      "['python', 'great', 'data', 'science', 'widely', 'used', 'ai', 'ml']\n",
      "\n",
      "Documents after spell correction:\n",
      "['loooove', 'nlp', 'soon', 'cool', 'nlp', 'fun', 'ai']\n",
      "['artificial', 'intelligence', 'future', 'technology']\n",
      "['python', 'great', 'data', 'science', 'widely', 'used', 'ai', 'ml']\n",
      "\n",
      "Word frequency before correction:\n",
      "[('nlp', 2), ('ai', 2), ('loooove', 1), ('sooo', 1), ('coool', 1)]\n",
      "\n",
      "Word frequency after correction:\n",
      "[('nlp', 2), ('ai', 2), ('loooove', 1), ('soon', 1), ('cool', 1)]\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# List of domain-specific words that shouldn't be corrected\n",
    "# These words might be specific to your domain or dataset\n",
    "custom_words = ['nlp', 'ai', 'ml']\n",
    "\n",
    "# Sample tokenized documents after stopword removal\n",
    "# In a real scenario, this would be the output from previous preprocessing steps\n",
    "tokenized_docs_no_stopwords = [\n",
    "    ['loooove', 'nlp', 'sooo', 'coool', 'nlp', 'fun', 'ai'],\n",
    "    ['artificial', 'intelligence', 'future', 'technology'],\n",
    "    ['python', 'great', 'data', 'science', 'widely', 'used', 'ai', 'ml']\n",
    "]\n",
    "\n",
    "# This prevents the spell checker from trying to correct these words\n",
    "spell.word_frequency.load_words(custom_words)\n",
    "\n",
    "# Function to correct spelling in a single document\n",
    "def correct_spelling(doc):\n",
    "    corrected_doc = []\n",
    "    for word in doc:\n",
    "        if word in custom_words:\n",
    "            # Skip spell check for custom words\n",
    "            corrected_doc.append(word)\n",
    "        else:\n",
    "            # Attempt to correct the word\n",
    "            corrected_word = spell.correction(word)\n",
    "            # If correction fails (returns None), keep the original word\n",
    "            corrected_doc.append(corrected_word if corrected_word else word)\n",
    "    return corrected_doc\n",
    "\n",
    "# Correct spelling mistakes in all documents\n",
    "corrected_docs = [correct_spelling(doc) for doc in tokenized_docs_no_stopwords]\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nOriginal documents:\")\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    print(doc)\n",
    "\n",
    "print(\"\\nDocuments after spell correction:\")\n",
    "for doc in corrected_docs:\n",
    "    print(doc)\n",
    "\n",
    "# Example of how spelling correction can affect word counts\n",
    "from collections import Counter\n",
    "\n",
    "print(\"\\nWord frequency before correction:\")\n",
    "before_correction = Counter([word for doc in tokenized_docs_no_stopwords for word in doc])\n",
    "print(before_correction.most_common(5))\n",
    "\n",
    "print(\"\\nWord frequency after correction:\")\n",
    "after_correction = Counter([word for doc in corrected_docs for word in doc])\n",
    "print(after_correction.most_common(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225e5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
